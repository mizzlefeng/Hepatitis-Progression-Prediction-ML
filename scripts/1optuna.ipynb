{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from src.common import init_logger, setup_seed\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "import json\n",
    "import itertools\n",
    "import optuna\n",
    "import os\n",
    "from tabpfn import TabPFNClassifier\n",
    "from model.deep_model import FCN, CNN,train_evaluate\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "def ml_optimize_model(X, y, model_name, feature_group, n_trials=50):\n",
    "    def objective(trial):\n",
    "        try:\n",
    "            params = {}\n",
    "            for param_name, param_info in config[model_name].items():\n",
    "                param_method = getattr(trial, param_info[\"type\"])\n",
    "                params[param_name] = param_method(param_name, *param_info[\"args\"])\n",
    "            \n",
    "            f1_list = []\n",
    "            for train_index, test_index in skf.split(X, y):\n",
    "                model_constructor = model_dict[model_name]\n",
    "                model = model_constructor(**params)\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                f1 = f1_score(y_test, y_pred)\n",
    "                f1_list.append(f1)\n",
    "            return np.mean(f1)\n",
    "        \n",
    "        except Exception as e:\n",
    "            trial.report(float('-inf'), step=0)\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    study_name = feature_group + \"_\" + model_name + \"_\" + MODE\n",
    "    try:\n",
    "        optuna.delete_study(study_name=study_name, storage=storage_name)\n",
    "        print(f\"Study '{study_name}' deleted successfully.\")\n",
    "    except:\n",
    "        print(f\"Study '{study_name}' does not exist, skipping deletion.\")\n",
    "    study = optuna.create_study(study_name=feature_group+\"_\"+model_name+\"_\"+MODE, storage=storage_name, direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=-1)\n",
    "    return study.best_params, study.best_value, study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "device = torch.device(f'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def dl_optimize_model(X, y, model_name, feature_group, n_trials=50):\n",
    "\n",
    "    def objective(trial):\n",
    "        if model_name == \"FCNN\":\n",
    "            lr = trial.suggest_loguniform('lr', 1e-4, 2e-1)\n",
    "            n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "            hidden_layers = [trial.suggest_categorical(f\"n_units_l{i}\", [8, 16, 32, 64, 128]) for i in range(n_layers)]\n",
    "            activation_func = trial.suggest_categorical('activation_func', ['relu','tanh','sigmoid'])\n",
    "            optimizer_name = trial.suggest_categorical('optimizer', ['AdamW','Adam','SGD'])\n",
    "            epochs = trial.suggest_int('epochs', 50, 500, step=10)\n",
    "            \n",
    "            f1_list = []\n",
    "            for train_index, test_index in skf.split(X, y):\n",
    "                X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "                model = FCN(input_dim=X.shape[1], output_dim=1, hidden_layers=hidden_layers, activation_func=activation_func).to(device)\n",
    "                optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "                criterion = nn.BCEWithLogitsLoss()\n",
    "                \n",
    "                f1_value,_,_ = train_evaluate(model, criterion, optimizer, X_train, y_train, X_val, y_val, epochs)\n",
    "                f1_list.append(f1_value)\n",
    "\n",
    "            return np.mean(f1_list)\n",
    "        \n",
    "        elif model_name == \"CNN\":\n",
    "            lr = trial.suggest_loguniform('lr', 1e-4, 2e-1)\n",
    "            n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "            n_filters = [trial.suggest_categorical(f\"n_filters_l{i}\", [8, 16, 32, 64, 128]) for i in range(n_layers)]\n",
    "            kernel_size = trial.suggest_categorical(\"kernel_size\", [3, 5, 7])\n",
    "            activation_func = trial.suggest_categorical('activation_func', ['relu','tanh','sigmoid'])\n",
    "            optimizer_name = trial.suggest_categorical('optimizer', ['AdamW','Adam','SGD'])\n",
    "            epochs = trial.suggest_int('epochs', 50, 500, step=10)\n",
    "            \n",
    "            f1_list = []\n",
    "            for train_index, test_index in skf.split(X, y):\n",
    "                X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "                model = CNN(n_features=X.shape[1], output_dim=1, n_layers=n_layers, n_filters=n_filters, kernel_size=kernel_size, activation_func=activation_func).to(device)\n",
    "                optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "                criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "                f1_value,_,_ = train_evaluate(model, criterion, optimizer, X_train, y_train, X_val, y_val, epochs)\n",
    "                f1_list.append(f1_value)\n",
    "\n",
    "            return np.mean(f1_list)\n",
    "\n",
    "        else:\n",
    "            NotImplementedError\n",
    "\n",
    "    study_name = feature_group + \"_\" + model_name + \"_\" + MODE\n",
    "    try:\n",
    "        optuna.delete_study(study_name=study_name, storage=storage_name)\n",
    "        print(f\"Study '{study_name}' deleted successfully.\")\n",
    "    except:\n",
    "        print(f\"Study '{study_name}' does not exist, skipping deletion.\")\n",
    "    study = optuna.create_study(study_name=study_name, storage=storage_name, direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=-1)\n",
    "    return study.best_params, study.best_value, study\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "def load_config(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "MODE = \"6M\"\n",
    "CV_RESULT = \"../result/01experiment/\"\n",
    "LOGFILE = f\"../result/01experiment/01model_optuna_{MODE}.log\"\n",
    "if os.path.exists(LOGFILE):\n",
    "    os.remove(LOGFILE)\n",
    "logger,file_handler = init_logger(LOGFILE)\n",
    "\n",
    "def my_svm(**params):\n",
    "    return SVC(probability=True, **params)\n",
    "\n",
    "def my_lgb(**params):\n",
    "    return LGBMClassifier(verbose=-1, **params)\n",
    "\n",
    "def my_tabpfn(**params):\n",
    "    return TabPFNClassifier(device='cuda',n_jobs=5, **params)\n",
    "\n",
    "model_dict = {\n",
    "    \"LR\": LogisticRegression,\n",
    "    \"SVM\": my_svm,\n",
    "    \"NB\": GaussianNB,\n",
    "    \"KNN\": KNeighborsClassifier,\n",
    "    \"RF\": RandomForestClassifier,\n",
    "    \"XGBoost\": XGBClassifier,\n",
    "    \"LightGBM\": my_lgb,\n",
    "    \"TabPFN\": my_tabpfn,\n",
    "}\n",
    "\n",
    "clinical_features = ['Gender', 'ALT', 'AST', 'Albumin', 'GGT', 'DBIL', 'IBIL', 'AFP', 'DNA load', 'HBsAg']\n",
    "specific_features = ['HBV-T', 'HBsAg-T(pH>7)', 'HBsAg-T(pH≤7)', 'HBpol-T(pH>7)', 'HBpol-T(pH≤7)', 'HBx-T(pH>7)', 'HBx-T(pH≤7)', 'HBeAg-T(pH>7)', 'HBeAg-T(pH≤7)']\n",
    "treat_features = ['ThSched', 'ADV', 'ETV', 'PEG-IFN', 'TAF', 'TDF', 'TFV', 'TMF']\n",
    "\n",
    "# Load data and config\n",
    "setup_seed(42)\n",
    "config = load_config('./config/ml_config.json')\n",
    "storage_name = \"postgresql://postgres:123...@127.0.0.1/hepatitis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../result/00pre-processing/05final_data-minmax.csv')\n",
    "y = data[f'{MODE}-Label']\n",
    "model_list = []\n",
    "\n",
    "feature_dict = {\n",
    "    \"CIF\": clinical_features,\n",
    "    \"STCF\": specific_features,\n",
    "    \"TPF\": treat_features\n",
    "}\n",
    "\n",
    "feature_names = list(feature_dict.keys())\n",
    "combinations_1 = list(itertools.combinations(feature_names, 1))\n",
    "combinations_2 = list(itertools.combinations(feature_names, 2))\n",
    "combinations_3 = list(itertools.combinations(feature_names, 3))\n",
    "all_combinations = combinations_1 + combinations_2 + combinations_3\n",
    "all_combinations\n",
    "\n",
    "for combo in all_combinations:\n",
    "\n",
    "    combined_features = []\n",
    "    for group in combo:\n",
    "        combined_features.extend(feature_dict[group])\n",
    "    feature_group = ' + '.join([g for g in combo])\n",
    "    logger.info(\"#\"*50)\n",
    "    logger.info(feature_group)\n",
    "    X = data[combined_features]\n",
    "    for model_name in model_dict.keys():\n",
    "        best_params, best_score, study = ml_optimize_model(X, y, model_name, feature_group, n_trials=50)\n",
    "        logger.info(f\"{model_name} Best parameters: {best_params}\")\n",
    "        logger.info(f\"{model_name} Best score: {best_score}\")\n",
    "    \n",
    "    for model_name in ['FCNN','CNN']:\n",
    "        best_params, best_score, study = dl_optimize_model(X, y, model_name, feature_group, n_trials=50)\n",
    "        logger.info(f\"{model_name} Best parameters: {best_params}\")\n",
    "        logger.info(f\"{model_name} Best score: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_handler.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mizzle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
